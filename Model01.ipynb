{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install Kaggle package\n",
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxjxWVjdEhdq",
        "outputId": "a0e1869e-6ec8-463e-f436-e418a62268fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.12)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.4)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ6fue6bElj0",
        "outputId": "7c6a9183-172c-4e6b-8b25-7585339b7c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory named 'kaggle' and copy kaggle.json file for authentication\n",
        "!mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "g-m04I0UEnXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "FLeO3UtrEr5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json  # Set file permissions"
      ],
      "metadata": {
        "id": "_kYfDHizEt7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Download the dataset\n",
        "!kaggle datasets download -d arashnic/book-recommendation-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_epKiphE710",
        "outputId": "1056e13d-0eec-41f2-9d54-5cfd92b62a96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset\n",
            "License(s): CC0-1.0\n",
            "Downloading book-recommendation-dataset.zip to /content\n",
            " 37% 9.00M/24.3M [00:00<00:00, 57.5MB/s]\n",
            "100% 24.3M/24.3M [00:00<00:00, 116MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Unzip the dataset\n",
        "!unzip book-recommendation-dataset"
      ],
      "metadata": {
        "id": "HeMHG4V-5Zk6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22423638-cb9f-4148-eabc-32e100aecf8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  book-recommendation-dataset.zip\n",
            "  inflating: Books.csv               \n",
            "  inflating: DeepRec.png             \n",
            "  inflating: Ratings.csv             \n",
            "  inflating: Users.csv               \n",
            "  inflating: classicRec.png          \n",
            "  inflating: recsys_taxonomy2.png    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "US9lDSEgFeso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read dataset files\n",
        "books = pd.read_csv('Books.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3XQTqOxFj4h",
        "outputId": "9cbf9da6-57d2-4a3e-fbba-e680409fac2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-4806dc8eda5b>:2: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  books = pd.read_csv('Books.csv')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(books.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAyaLjJdFrNs",
        "outputId": "c34ce2e8-5fe1-4345-e9cb-31f24f1369ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         ISBN                                         Book-Title  \\\n",
            "0  0195153448                                Classical Mythology   \n",
            "1  0002005018                                       Clara Callan   \n",
            "2  0060973129                               Decision in Normandy   \n",
            "3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
            "4  0393045218                             The Mummies of Urumchi   \n",
            "\n",
            "            Book-Author Year-Of-Publication                   Publisher  \\\n",
            "0    Mark P. O. Morford                2002     Oxford University Press   \n",
            "1  Richard Bruce Wright                2001       HarperFlamingo Canada   \n",
            "2          Carlo D'Este                1991             HarperPerennial   \n",
            "3      Gina Bari Kolata                1999        Farrar Straus Giroux   \n",
            "4       E. J. W. Barber                1999  W. W. Norton &amp; Company   \n",
            "\n",
            "                                         Image-URL-S  \\\n",
            "0  http://images.amazon.com/images/P/0195153448.0...   \n",
            "1  http://images.amazon.com/images/P/0002005018.0...   \n",
            "2  http://images.amazon.com/images/P/0060973129.0...   \n",
            "3  http://images.amazon.com/images/P/0374157065.0...   \n",
            "4  http://images.amazon.com/images/P/0393045218.0...   \n",
            "\n",
            "                                         Image-URL-M  \\\n",
            "0  http://images.amazon.com/images/P/0195153448.0...   \n",
            "1  http://images.amazon.com/images/P/0002005018.0...   \n",
            "2  http://images.amazon.com/images/P/0060973129.0...   \n",
            "3  http://images.amazon.com/images/P/0374157065.0...   \n",
            "4  http://images.amazon.com/images/P/0393045218.0...   \n",
            "\n",
            "                                         Image-URL-L  \n",
            "0  http://images.amazon.com/images/P/0195153448.0...  \n",
            "1  http://images.amazon.com/images/P/0002005018.0...  \n",
            "2  http://images.amazon.com/images/P/0060973129.0...  \n",
            "3  http://images.amazon.com/images/P/0374157065.0...  \n",
            "4  http://images.amazon.com/images/P/0393045218.0...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "users = pd.read_csv('Users.csv')"
      ],
      "metadata": {
        "id": "DkPB-foOFy8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(users.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERx_HNwLF5OP",
        "outputId": "72a5df60-65ec-4d6b-8117-574499a8f437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   User-ID                            Location   Age\n",
            "0        1                  nyc, new york, usa   NaN\n",
            "1        2           stockton, california, usa  18.0\n",
            "2        3     moscow, yukon territory, russia   NaN\n",
            "3        4           porto, v.n.gaia, portugal  17.0\n",
            "4        5  farnborough, hants, united kingdom   NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = pd.read_csv('Ratings.csv')"
      ],
      "metadata": {
        "id": "JNNDx8WRGCs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ratings.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqrwS3_MChsg",
        "outputId": "c50da415-129b-40fb-9044-c0ef0d269448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   User-ID        ISBN  Book-Rating\n",
            "0   276725  034545104X            0\n",
            "1   276726  0155061224            5\n",
            "2   276727  0446520802            0\n",
            "3   276729  052165615X            3\n",
            "4   276729  0521795028            6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge datasets based on common column (ISBN)\n",
        "merged_data = pd.merge(ratings, books, on='ISBN')\n",
        "merged_data = pd.merge(merged_data, users, on='User-ID')\n",
        "print(merged_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfuElcEKGSb5",
        "outputId": "96602fec-6ae5-47dd-831d-d9f9ed21ceb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   User-ID        ISBN  Book-Rating  \\\n",
            "0   276725  034545104X            0   \n",
            "1     2313  034545104X            5   \n",
            "2     2313  0812533550            9   \n",
            "3     2313  0679745580            8   \n",
            "4     2313  0060173289            9   \n",
            "\n",
            "                                         Book-Title       Book-Author  \\\n",
            "0                              Flesh Tones: A Novel        M. J. Rose   \n",
            "1                              Flesh Tones: A Novel        M. J. Rose   \n",
            "2     Ender's Game (Ender Wiggins Saga (Paperback))  Orson Scott Card   \n",
            "3             In Cold Blood (Vintage International)     TRUMAN CAPOTE   \n",
            "4  Divine Secrets of the Ya-Ya Sisterhood : A Novel     Rebecca Wells   \n",
            "\n",
            "  Year-Of-Publication         Publisher  \\\n",
            "0                2002  Ballantine Books   \n",
            "1                2002  Ballantine Books   \n",
            "2                1986         Tor Books   \n",
            "3                1994           Vintage   \n",
            "4                1996     HarperCollins   \n",
            "\n",
            "                                         Image-URL-S  \\\n",
            "0  http://images.amazon.com/images/P/034545104X.0...   \n",
            "1  http://images.amazon.com/images/P/034545104X.0...   \n",
            "2  http://images.amazon.com/images/P/0812533550.0...   \n",
            "3  http://images.amazon.com/images/P/0679745580.0...   \n",
            "4  http://images.amazon.com/images/P/0060173289.0...   \n",
            "\n",
            "                                         Image-URL-M  \\\n",
            "0  http://images.amazon.com/images/P/034545104X.0...   \n",
            "1  http://images.amazon.com/images/P/034545104X.0...   \n",
            "2  http://images.amazon.com/images/P/0812533550.0...   \n",
            "3  http://images.amazon.com/images/P/0679745580.0...   \n",
            "4  http://images.amazon.com/images/P/0060173289.0...   \n",
            "\n",
            "                                         Image-URL-L               Location  \\\n",
            "0  http://images.amazon.com/images/P/034545104X.0...      tyler, texas, usa   \n",
            "1  http://images.amazon.com/images/P/034545104X.0...  cincinnati, ohio, usa   \n",
            "2  http://images.amazon.com/images/P/0812533550.0...  cincinnati, ohio, usa   \n",
            "3  http://images.amazon.com/images/P/0679745580.0...  cincinnati, ohio, usa   \n",
            "4  http://images.amazon.com/images/P/0060173289.0...  cincinnati, ohio, usa   \n",
            "\n",
            "    Age  \n",
            "0   NaN  \n",
            "1  23.0  \n",
            "2  23.0  \n",
            "3  23.0  \n",
            "4  23.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values\n",
        "merged_data['Age'].fillna(merged_data['Age'].mean(), inplace=True)\n",
        "merged_data['Location'].fillna('Unknown', inplace=True)  # Fill missing Location with 'Unknown'\n",
        "merged_data['Publisher'].fillna('Unknown', inplace=True) # Fill missing Publisher with 'Unknown'\n"
      ],
      "metadata": {
        "id": "y_hP_cJkKlIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n"
      ],
      "metadata": {
        "id": "LzJqJqf7g_Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(merged_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8mYH9wbiUjQ",
        "outputId": "c69c7f69-b41a-41df-d65a-b13de92b3281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   User-ID        ISBN  Book-Rating  \\\n",
            "0   276725  034545104X            0   \n",
            "1     2313  034545104X            5   \n",
            "2     2313  0812533550            9   \n",
            "3     2313  0679745580            8   \n",
            "4     2313  0060173289            9   \n",
            "\n",
            "                                         Book-Title       Book-Author  \\\n",
            "0                              Flesh Tones: A Novel        M. J. Rose   \n",
            "1                              Flesh Tones: A Novel        M. J. Rose   \n",
            "2     Ender's Game (Ender Wiggins Saga (Paperback))  Orson Scott Card   \n",
            "3             In Cold Blood (Vintage International)     TRUMAN CAPOTE   \n",
            "4  Divine Secrets of the Ya-Ya Sisterhood : A Novel     Rebecca Wells   \n",
            "\n",
            "  Year-Of-Publication         Publisher  \\\n",
            "0                2002  Ballantine Books   \n",
            "1                2002  Ballantine Books   \n",
            "2                1986         Tor Books   \n",
            "3                1994           Vintage   \n",
            "4                1996     HarperCollins   \n",
            "\n",
            "                                         Image-URL-S  \\\n",
            "0  http://images.amazon.com/images/P/034545104X.0...   \n",
            "1  http://images.amazon.com/images/P/034545104X.0...   \n",
            "2  http://images.amazon.com/images/P/0812533550.0...   \n",
            "3  http://images.amazon.com/images/P/0679745580.0...   \n",
            "4  http://images.amazon.com/images/P/0060173289.0...   \n",
            "\n",
            "                                         Image-URL-M  \\\n",
            "0  http://images.amazon.com/images/P/034545104X.0...   \n",
            "1  http://images.amazon.com/images/P/034545104X.0...   \n",
            "2  http://images.amazon.com/images/P/0812533550.0...   \n",
            "3  http://images.amazon.com/images/P/0679745580.0...   \n",
            "4  http://images.amazon.com/images/P/0060173289.0...   \n",
            "\n",
            "                                         Image-URL-L               Location  \\\n",
            "0  http://images.amazon.com/images/P/034545104X.0...      tyler, texas, usa   \n",
            "1  http://images.amazon.com/images/P/034545104X.0...  cincinnati, ohio, usa   \n",
            "2  http://images.amazon.com/images/P/0812533550.0...  cincinnati, ohio, usa   \n",
            "3  http://images.amazon.com/images/P/0679745580.0...  cincinnati, ohio, usa   \n",
            "4  http://images.amazon.com/images/P/0060173289.0...  cincinnati, ohio, usa   \n",
            "\n",
            "         Age  \n",
            "0  37.397648  \n",
            "1  23.000000  \n",
            "2  23.000000  \n",
            "3  23.000000  \n",
            "4  23.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode 'ISBN' and 'User-ID' using Label Encoding\n",
        "merged_data['ISBN'] = label_encoder.fit_transform(merged_data['ISBN'])\n",
        "merged_data['User-ID'] = label_encoder.fit_transform(merged_data['User-ID'])\n",
        "# Perform One-Hot Encoding for 'Location' and 'Publisher'\n",
        "merged_data = pd.get_dummies(merged_data, columns=['Location', 'Publisher'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "2lJe8aPWixIU",
        "outputId": "72e50690-0b60-412b-c998-58315e3b1736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'label_encoder' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d2ecfecc2515>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Encode 'ISBN' and 'User-ID' using Label Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmerged_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ISBN'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ISBN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmerged_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'User-ID'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'User-ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Perform One-Hot Encoding for 'Location' and 'Publisher'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmerged_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Location'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Publisher'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'label_encoder' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = train_test_split(merged_data, test_size=0.2, random_state=42)\n",
        "print(\"Number of training samples:\", len(train_data))\n",
        "print(\"Number of testing samples:\", len(test_data))"
      ],
      "metadata": {
        "id": "tB1Nu8IyFJA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hide_ratings(data):\n",
        "    # Filter users with fewer than 3 ratings\n",
        "    users_with_enough_ratings = data['User-ID'].value_counts()[data['User-ID'].value_counts() >= 3].index\n",
        "    data_filtered = data[data['User-ID'].isin(users_with_enough_ratings)]\n",
        "    # Sample 3 items for each user\n",
        "    hidden_ratings = data_filtered.groupby('User-ID').apply(lambda x: x.sample(n=3, random_state=42))\n",
        "    visible_ratings = data_filtered[~data_filtered.index.isin(hidden_ratings.index)]\n",
        "    return visible_ratings, hidden_ratings\n",
        "\n",
        "train_visible, train_hidden = hide_ratings(train_data)\n",
        "test_visible, test_hidden = hide_ratings(test_data)\n",
        "\n",
        "print(\"Number of visible training samples:\", len(train_visible))\n",
        "print(\"Number of hidden training samples:\", len(train_hidden))\n",
        "print(\"Number of visible testing samples:\", len(test_visible))\n",
        "print(\"Number of hidden testing samples:\", len(test_hidden))\n"
      ],
      "metadata": {
        "id": "cKsL6UlwFVJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Filter users who have rated 7 or more books\n",
        "user_ratings_count = merged_data['User-ID'].value_counts()\n",
        "users_with_enough_ratings = user_ratings_count[user_ratings_count >= 7].index\n",
        "filtered_data = merged_data[merged_data['User-ID'].isin(users_with_enough_ratings)]\n",
        "\n",
        "# Randomly select 100 users\n",
        "selected_users = np.random.choice(users_with_enough_ratings, size=100, replace=False)\n",
        "\n",
        "# Split data for selected users into visible and hidden samples\n",
        "visible_data = filtered_data[filtered_data['User-ID'].isin(selected_users)].copy()\n",
        "hidden_data = visible_data.groupby('User-ID').apply(lambda x: x.sample(n=3, random_state=42)).reset_index(drop=True)\n",
        "visible_data = visible_data[~visible_data.index.isin(hidden_data.index)]\n",
        "\n",
        "print(\"Number of visible samples:\", len(visible_data))\n",
        "print(\"Number of hidden samples:\", len(hidden_data))"
      ],
      "metadata": {
        "id": "bocbZQnkFj1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "isbn_encoder = OneHotEncoder(sparse=False)\n",
        "user_encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "X_books = isbn_encoder.fit_transform(visible_data[['ISBN']])\n",
        "X_users = user_encoder.fit_transform(visible_data[['User-ID']])\n",
        "\n",
        "X = np.concatenate([X_books, X_users], axis=1)"
      ],
      "metadata": {
        "id": "dsoFtnqPFmbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of features in one-hot encoded data\n",
        "num_features_books = X_books.shape[1]\n",
        "num_features_users = X_users.shape[1]\n",
        "total_features = num_features_books + num_features_users\n",
        "print(\"Total number of features:\", total_features)\n"
      ],
      "metadata": {
        "id": "whk80fOYHQPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Verify Input Data Dimensions\n",
        "print(\"Training data shape:\", train_data.shape)\n",
        "print(\"Testing data shape:\", test_data.shape)\n",
        "\n",
        "# 2. Check Model Input Shape\n",
        "print(\"Expected input shape for the model:\", model.input_shape)\n",
        "\n",
        "# 3. Inspect Data Preprocessing\n",
        "# Ensure that your data preprocessing steps do not alter the input dimensions unexpectedly.\n",
        "# If you're using any transformations or scaling, verify that they preserve the shape of the input data.\n",
        "\n",
        "# 4. Check for Data Integrity\n",
        "# Check for missing values or anomalies in your data that could affect input dimensions.\n",
        "print(\"Number of missing values in training data:\", np.isnan(train_data).sum())\n",
        "print(\"Number of missing values in testing data:\", np.isnan(test_data).sum())\n"
      ],
      "metadata": {
        "id": "3lZeUaHGHqx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate visible and hidden data for fitting the encoder\n",
        "all_data = pd.concat([visible_data, hidden_data_grouped])\n",
        "\n",
        "# One-hot encode categorical features\n",
        "X_books_all = isbn_encoder.fit_transform(all_data[['ISBN']])\n",
        "X_users_all = user_encoder.fit_transform(all_data[['User-ID']])\n",
        "\n",
        "# Split the one-hot encoded data back into visible and hidden parts\n",
        "X_books_visible = X_books_all[:len(visible_data)]\n",
        "X_books_hidden = X_books_all[len(visible_data):]\n",
        "X_users_visible = X_users_all[:len(visible_data)]\n",
        "X_users_hidden = X_users_all[len(visible_data):]\n",
        "\n",
        "# Concatenate the encoded features\n",
        "X_visible = np.concatenate([X_books_visible, X_users_visible], axis=1)\n",
        "X_hidden = np.concatenate([X_books_hidden, X_users_hidden], axis=1)\n",
        "\n",
        "# Predict ratings for the hidden data\n",
        "hidden_y_pred = model.predict(X_hidden)\n",
        "\n",
        "# Compute RMSE and MSE\n",
        "rmse = np.sqrt(mean_squared_error(hidden_ratings, hidden_y_pred))\n",
        "mse = mean_squared_error(hidden_ratings, hidden_y_pred)\n",
        "\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n"
      ],
      "metadata": {
        "id": "imS8J8VnGktc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "input_shape = X.shape[1]\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(input_shape,)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "y = visible_data['Book-Rating']\n",
        "\n",
        "model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "hidden_data_grouped = hidden_data.groupby('User-ID').head(3)\n",
        "\n",
        "hidden_X_books = isbn_encoder.transform(hidden_data_grouped[['ISBN']])\n",
        "hidden_X_users = user_encoder.transform(hidden_data_grouped[['User-ID']])\n",
        "hidden_X = np.concatenate([hidden_X_books, hidden_X_users], axis=1)\n",
        "hidden_y_pred = model.predict(hidden_X)\n",
        "\n",
        "hidden_ratings = hidden_data_grouped['Book-Rating'].values\n",
        "\n",
        "comparison_df = pd.DataFrame({'User-ID': hidden_data_grouped['User-ID'], 'Hidden Rating': hidden_ratings, 'Predicted Rating': hidden_y_pred.flatten()})\n",
        "print(comparison_df)"
      ],
      "metadata": {
        "id": "QBjf5C8AFuEY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}